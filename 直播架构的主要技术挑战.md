### 1、引言

众所周知，直播行业这几年非常的火爆，比如平时我们都在用的B站的直播，那我们如何去保证在直播当中的一些弹幕的实时性？

本章我们就来聊一下关于直播过程当中它的一些技术挑战和实现思路。

### 2、直播架构

对于直播的架构，其实在业界已经有非常成熟的解决方案了，简单来说直播架构主要分为三大模块：

![img](https://voximplant.com/assets/images/2020/10/30/rtmp-scheme-1.jpg)

[^RTMP 如何促进直播]: https://voximplant.com/blog/how-rtmp-facilitates-live-streaming

- 第一个模块就是客户端，其中客户端模块又分为主播端和观众端。
- 第二个模块就是应用服务器集群，里面包含大部分的业务处理逻辑。
- 第三个模块就是CND边缘节点技术。

### 3、模块交互

搞清楚了直播的架构和模块的划分，那模块之间又是如何交互的呢？具体步骤如下：![截屏2023-01-05 下午4.45.58](/Users/sghl/Library/Application Support/typora-user-images/截屏2023-01-05 下午4.45.58.png)

- 首先，主播的客户端这边先开启一个直播间，然后获取到CDN的一个推流地址。
- 其次，获取到推流地址之后通过传输协议把它推流到CDN的服务器上。
- 最后，观众通过一定的业务逻辑进入到直播间之后它也能获取到这个CDN的地址，然后通过CDN的地址去拿到自己的一个视频流。

### 4、技术挑战

其实对于直播涉及到的技术挑战是非常多的，因为直播也是分类型的：

- 实时型：B站直播会涉及到弹幕的实时性和互动性交互；
- 风控型：电商直播会涉及到高并发的一些秒杀场景；

所以说虽然说都是直播，但是它们的侧重点其实是不一样的。我认为直播最大的挑战还是在它的实时性和它的互动性。

对于实时性的要求主要分为两部分：

- 推流：数据在内部网络进行分发，目前主流的推流协议有三种：RTMP、HTTP-FLV、HLS；
- 拉流：客户端主动从CDN边缘节点拉取视频流。

### 5、推流协议

推流是指在内部网络进行分发，目前主流的推流协议主要有三种：

| 协议     | 传输协议 | 视频封装格式 | 延时 | 数据分段 | HTML5  | 应用场景                       |
| -------- | -------- | ------------ | ---- | -------- | ------ | ------------------------------ |
| RTMP     | TCP      | flv tag      | 2s   | 连续流   | 不支持 | 互动直播、点播（在线教育）     |
| HTTP-FLV | HTTP     | flv          | 2s   | 连续流   | 支持   | H5、互动直播、点播（在线教育） |
| HLS      | HTTP     | m3u8/ts      | 10s  | 切片     | 支持   | H5、直播、点播（在线教育）     |

- RTMP：是Adobe基于Flash传输研发出来的一个协议，它是基于TCP的视频流，所以它的可靠性和稳定性是比较高的，但是也因为是TCP接口，很多公司对于TCP的管控的也比较严格，所以这对于一些防火墙就不是特别友好，而且它主要是基于flash进行研发的，所以像苹果很多设备和windows的一些平台它都不支持这种flash播放器。针对上面的问题Adobe推出了新的协议HTTP-FLV协议。
- HTTP-FLV：目前直播当中用的最多的协议。
- HLS：是苹果主导的一个传输协议，这个协议和其它协议不太一样，它不是这种流式的传输，它是将文件切分成小的数据包，分片传输了之后再在服务端做数据的组装，形成一个完整的文件。虽然这个协议是苹果主导的，但是现在很多Android的播放器它都是支持的，但是这个协议有一个比较大弊端，就是它的延迟会比较大，像B站直播这种平台比较强调互动性的应用场景就不太适合这种协议；

综上所述，推流一般是采用这种RTMP协议，因为它的可靠性会比较高，大部分场景对于音频这一块数据的来源一定要保证它的高质量。

### 6、拉流协议

拉流是指客户端主流从CDN节点拉取视频流的过程，那对于数据的拉取这一块，对于观察端（客户端）来说它的场景非常的复杂，涉及到非常多的终端还有不同的网络场景，所以它要根据不同的情况去选择一些不同的策略。

例如，在不同的场景它的网络信号可能就不太一样，所以你要实时的去调整它的分辨率，所以当视频流推流到CDN节点上面的时候，我们还要进行不同的编码，像我们现在主流的编码就是H.264。

虽然，现在也有更加高效的像H.265这种编码格式，它能够在更低码率的情况下保证同样的视频质量，但是这个编码的复杂度也是非常高的，所以，对于服务端的计算资源消耗还是比较大的，需要结合各种资源（计算资源和网络带宽）情况（性价比、业务场景）综合考虑。

<img src="https://suggestionofmotion.com/wp-content/uploads/Codec_Comparison_File-size_H264-H265.png" alt="H.265 vs H.264 - Suggestion of Motion"  />

另外，就是H.265这种编码格式也需要客户和硬件的支持的。

<img src="http://68.media.tumblr.com/d3ee7a34db5eadad6b8edd80bae51fb5/tumblr_inline_osfnbwMeGW1qg4bow_540.png" alt="img" style="zoom:200%;" />

对于带宽这个事情，如果你的直播人数特别多CDN的流量成本可能还是比较高的。当然，现在也有一些其它的技术去节省这个带宽，比如说使用P2P这种点对点的技术去传输，如下图所示：

![What's the difference between peer-to-peer (P2P) networks and client-server?  | Resilio Blog](https://blog.resilio.com/wp-content/uploads/2018/06/p2p-vs-client-server-architecture.jpg)

P2P点对点传输并不是所有的流量都走CDN的这种边缘节点，你也可以通过一些其它的终端去进行一个流量的上传。虽然P2P这个技术出现的很早，而且这个想法也是非常好，但是在节省这个运营方成本的时候，其实也是占用了用户的带宽。

作为一个普通用户来说会有一种被白嫖的感觉，理想情况是我为人人，人人为我，但是实际情况就是希望大家都为我贡献带宽，我并不想贡献自己的带宽。对于这一点如果是你你会限制自己的上行带宽吗？

![What are the advantages of P2P exchange platforms?](https://aivia.io/blog/en/content/images/size/w1920/2021/06/affa.jpg)

除了上面说的这些技术，其实还有一些其它的优化方案可以降低延时，比如像我们传统的DNS域名解析都是通过这种DNS服务器，DNS依赖运营商的一些递归去查找，如下图所示：

![What is an Origin Server | Origin vs Edge Server | CDN Guide | Imperva](https://www.imperva.com/learn/wp-content/uploads/sites/13/2019/01/authoritative-dns-server.png)

这个一旦选定了这个CDN的运营商之后，可以通过服务端同步调用接口的方式拿到它对应的CDN地址，这样就不用跨网段去请求运营商的DNS服务器，如下图所示：

![A Look into the Connection Between CDNs and EDNS](https://assets-global.website-files.com/5babb9f91ab233ff5f53ce10/608e8ed4d3b019517ce56a27_edns6.png)

### 7、风控策略

对于B站的这种直播的应用场景其实还有一些其它的技术挑战，比如：风控策略如何做？在直播互动的这个过程当中如何保证这个画面实时的审核，还有这些弹幕它的一些敏感词的过滤？

对于B站的应用场景它主要结合了一些机器AI自动识别技术和人工的识别。简单来说它可能通过一些初步的阈值，比如说你是一个很小的直播间或者说你过往的直播记录很好，它可能就完全通过机器去审核（图像识别）进行一个自动的审核。

那什么情况下会进进入人工审核队列呢？

对于那种直播间人数到达一定阈值的时候或者说机器识别判定有一定的风险，这个时候可能就会触发到人工审核队列里面进行一个二次的审核。

### 8、弹幕实时性

基于B站的应用场景，我们如何保证评论的一个实时性要求呢？当用户发出一个评论，如何让别的用户尽快能够看得到？

![截屏2023-01-05 下午4.42.06](/Users/sghl/Library/Application Support/typora-user-images/截屏2023-01-05 下午4.42.06.png)

首先，要保证一定的实时性最好的方式就是采用长连接，对于长连接的管理需要一个会话的集群。而集群之间如何去路由，比如说直播间涉及到哪些用户，这里面肯定需要维护一个映射关系的共享存储，这里可以采用Redis这种解决方案，如下图所示：

![在云中扩展Websocket(第1 部分)。从Socket.io 和Redis 到Docker 和Kubernetes  的分布式架构_docker_weixin_0010034-DevPress官方社区](https://res.cloudinary.com/practicaldev/image/fetch/s--DxtykuUC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://raw.githubusercontent.com/sw360cab/websockets-scaling/master/diagrams/multi_server_proxied.png)

其次，在推送弹幕的时候，如果这个客户端比较多你要保证它的实时性要采用多线程并行的一些技术，而且对于弹幕这种应用场景并不要求每一条弹幕都要触达，这个允许的一定的数据丢失的。所以，可以大胆的采用异步的解决方案且产品上也可以优化的地方，比如1s最多只能展示100个弹幕，因为超过100个其实没有太多意义，这个时候我们可以随机的选取100条就够了。

最后，真正遇到这些技术挑战的话也可以和产品进行沟通进行优化或取舍，不要只局限于技术的角度上去解决问题。

除了上面的这些问题外，还有一些数据方面的挑战，比如说直播间的人数有多少？ 对于直播人数的这种数据很多时候退出的时候并不是正常退出的，它并不一定能调用接口。另外，这个数据它对于准确性的要求也不是很高，所以只需要保持一个心跳，定期的去监测一下去刷新一个这个数据就可以了。

综上所述，直播最主要的挑战主要是它的实时性和互动性。

