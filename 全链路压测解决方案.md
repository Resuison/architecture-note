### 前言

全链路压测是一个非常系统性的工程，它涉及到运营、产品、开发、测试、运维。本章我会基于我自已在实际工作当中的一些实践经验与大家分享我在做整件事情当中遇到一些问题和思考，并结合实际的业务场景给出的解决方案和应对策略。

### 前期准备

在大促前期，为了应对大促的活动要着手准备全链路压测的事情。从这个阶段开始针对于一些需求的排期它都要为这个去进行让步。

尤其是开始压测之后，整个线上系统的变更就要非常的谨慎，避免过多的需求对压测的结果进行一个干扰，这也就是我们常说的需求封版。 针对于一些大促或者一些比较重要的事情，我们都会提前一段时间去进行需求的封版。

首先，我们先来分析一下什么系统需要去做压测？

### 什么系统需要去做压测？

一句话概括，就是这种突发流量并且对可用性要求比较高的这种系统，比如：12306、健康码、双11活动和一些金融类的一些系统。

你会发现这些系统往往是TOC的，它的用户量、并发量会比较高，但是它对可靠性的要求又非常高，比如说现在每天大家都在扫健康码，每天上班进出一些大楼它都需要扫，那这种就是峰值。就是要保证这种突发的峰值来了之后，我们的系统还是保证一个稳定的高可用状态。

### 如何选择压测环境？

分析完了哪些系统需要做压测后，我们就需要对环境进行一个选择，选择在什么样的环境下进行压测？

一般来说互联网公司都有几套环境的（dev,test,pre,prod），绝大部分的压测环境都会选择生产环境进行。也许你会觉得很奇怪为什么不搭建一套和线上环境一模一样的环境去进行压测。

理论上来讲的确应该这样去做，但是大公司虽然有钱但维护的集群规模和各种运维的中间件成本也是非常高的，是难以保证完全跟线上一样的环境。而且很多压测它和历史数据的规模是存在一定的关系的，所以还需要离线的去构造这些压测数据也是一件非常麻烦的事情。

 由于，在线上压测就是怕对线上造成影响，这个时候就必须保证以下人员必须在现场，比如：开发、测试。所以，整体压测的一个节奏是完全可控的， 一旦出现这种失败率飙高的情况也可以马上停止压测，开发人员也能够及时响应处理问题， 总体来说是全局可控的。

所以，相比去搭建一套跟线上一模一样的环境，而且在结果不一定可靠的情况下，这样直接去线上进行压测这是一个更好的选择。 

当然，在实践过程当中的确在线上压测遇到过一些故障，但是总体来说这也能够算到这个压测本身的成本中。

接下，我们就来详细说明一下整个压测需要经历哪些步骤？包括它每个步骤需要具体做哪些事情？

### 压测步骤

总体来说整个压测需要经历以下几个步骤：

**第一点：事先梳理业务**

我们在每次压测之前，需要明确我们的一个业务目标是什么？比如说通过运营产品用户上的一些指标去进行一些反推，我们系统大概需要怎么样的一个水位？ 所以，这个阶段要充分的跟运营产品去进行一个沟通，他们具体要搞什么样的活动、大概持续的时间、用户量大概是多少，然后也要结合一些历史数据，无论是从性能还是业务上去结合这些东西综合的给出一个系统需要的一个指标。

比如，以电商的场景为例， 我们会分析不同的页面它的用户访问流量的占比，还有打开商品详情页查看和下单的这些流量的占比。甚至说不同的商品它下单走的链路可能是不一样的，那我们还要去梳理不同的业务链路它的一个流量的占比大概是怎么样的？

然后，我们还要去分析它的一个核心链路，哪些服务是非核心的，我们是不是就可以去给它做服务的降级处理？哪些服务是不是要去做扩容处理？数据库是否要升级等等。

**第二点：确定压测工具**

梳理了前面的业务后，接下来就是要确定使用什么样的压测工具和压测数据如何去准备？ 虽然，现在有一些Jmeter或者说wrk这样的压测工具，但是整个来说我们真的要去做全链路压测还是要用一些回放流量、流量引擎这种东西去混合的多接口进行压测，混合接口压测和单接口压测还是非常不一样的。 

**第三点：对数据进行预热**

确定好了压没工具之后，接下来就是要对哪些数据进行预热处理？比如说有很多商品它在大促的12点就要马上放出来了，那这个点很多数据还是冷数据，是不是应该提前把这些数据加载到内存里面，避免同时大量的用户去访问DB（数据库）。

比如，以电商的场景为例，我们会在大促前1个小时左右将所有参与活动的商品数据（图片、库存）会进行预热处理，图片会放在CDN节点上，商品库存数据会放在Redis和JVM内存。

**第四点：调整应用配置**

准备好了一切之后，接下来就是对限流的配置进行调整，在大促期间把限流放开，适当的调高一些，以保证系统的稳定性和可靠性。

### 链路改造

对于前面所有的工作都是属于梳理和准备工作，接下来我们会针对大促期间的链路改造大概会涉及到哪些东西进行全面的讲解。

**对代码进行Review**

经过前面的梳理对于核心链路，每个应用它都是有相关的负责人的，此时应该找到每一个应用或服务的负责人先进行代码review，负责人必须清楚的知道核心链路有哪些点需要优化？ 比如说：可不可能存在一些慢SQL、一些大事务或者说一些线程数的优化等等。

**对流量进行隔离**

其次，就是读写流量的分离，为什么要去做这个事情，因为读的流量还好，但是写入的流量必须要跟线上做一个隔离，否则会把线上的数据给搞乱了。当然，有一些场景可以通过业务上的标识去进行一个区分，比如说：用一个测试账号进行压测，压测完成之后把测试账号的数据进行清理。但是，有些数据它没办法做到业务上的隔离，那这个时候就需要改造一些中间件的方式进行处理。比如说使用Agent或AOP的方式去进行一个流量的切分处理。在数据库层面就要去设计一张影子表或者测试库，然后把这种压测的流量放到一个影子表里面去，最后再进行一个统一的清理。当然，如果你懒得去建影子这种东西，它可能到这一层就直接去进行一个熔断或者返回一个mock的数据。 这样做当然是不真实的，因为这样可能没有压测到DB能不能扛住这个压力。

![img](http://assets.processon.com/chart_image/636d21166376897f2b166f4d.png)

### 如何实施

**创建场景，逐步放量**

前面的准备好了之后，我们就开始做压测，在压测的过程当中不管理开发、测试相关的人员一定要到场，因为一旦出现一些紧急情况马上能够进行一个应急响应，压测一般是QA测试去进行实施的，流量也是一个逐步爬坡的过程（从小到大），通过流量引擎请求了rps，一旦这个接口响应变长，它就会把这个速率给放慢。

![img](https://docs.shulie.io/uploads/forcecop/images/m_601ca4004aeb1677c276cff3c7facec3_r.png)

![img](https://docs.shulie.io/uploads/forcecop/images/m_e395bb18bd82c277d12ef2dda666f0b7_r.png)

1.点击新增压测场景按钮，打开新增压测场景页面；
2.压测场景支持设置定时启动，以及关闭定时任务。设置了定时任务，仍然可以手动启动当前压测场景，而且不会影响定时任务。
3.压测场景支持增加便签，和通过标签搜索。
4.输入压测场景名称，可自定义；
5.在编辑压测场景时可以设置定时启动，定时的最小时间必须大于保存压测场景时刻 1 分钟。
6.设置本次需要压测的业务活动，
①仅可选择状态为正常的业务活动，若需要压测的业务活动无法选择请先确认状态是否正常；
②需要给每个业务活动设定指标的目标值，包括：TPS、RT、成功率、SA（RT的达标率）；
③点击➕可添加多个业务活动；
7.设置本次的施压模式，
①设置最大的并发数；
使用固定施压模式时，将以此并发数固定施压
使用线性递增和阶梯递增模式施压时，将以此并发数为最大并发数标准增压。
②设置施压的IP数，根据并发数的设定，系统会给出可选择的IP数量范围区间，指定区间内的某个数值即可；
③设置压测时长，默认单位为分钟，可切换为小时；
④设置施压模式，目前支持三种模式：
[1]固定压力值：以设置的最大并发数恒定施压模式；
[2]线性递增：以线性增压直至最大并发的施压模式，需要设置递增时长，默认单位为分钟，可切换为小时，不可超过压测时长；
[3]阶梯递增：以阶梯增压直至最大并发的施压模式，需要设置递增时长和递增层数，每层的施压时间=递增时长/递增层数，递增时长默认单位为分钟，可切换为小时，不可超过压测时长；
8.SLA配置：压测的特殊协议约定，可设置某些指标出现异常结果时进行报警提醒或立即终止压测，
①输入规则名称，可自定义；
②选择对象，可选择全部或单个业务活动；
③选择指标，可从TPS、RT、成功率、SA中选择；
④设置触发条件和阈值；
⑤设置触发次数；
⑥点击➕可添加多条SLA规则；
⑦终止条件为必填，至少需要一条规则，告警条件可不填；
9.点击保存，保存当前压测场景配置。

**发现瓶颈，逐步优化**

作为开发人员要时刻关注大盘，观察有没有报错，查看整个请求的一个火焰图和链路的耗时。这个时间的话有一定的持续时间，因为有些中间件，比如说：消息队列它可能是在堆积的过程当中，它需要持续一段时间才能暴露出它整个性能的瓶颈。正常情况下压测总能遇到一个瓶颈，那这个瓶颈就看一下能不能满足业务的指标，如果不能满足，看这个瓶颈在什么地方，相关的人员就进行一些优化，优化之后再进行实施，然后再去优化，再去实施，循环整个过程。最后，直到能够达到我们业务的要求为止。

![img](http://assets.processon.com/chart_image/636daf196376897f2b16d97e.png)

**混沌工程，故障演练**

完成了整个过程，你可能会认为这样就结束了，但是这远远不够，上面所有做的事情其实只是知道我们系统它的极限值大概在哪里，那我们应该还要有一些兜底的方案和策略，要去进行一些配置的变更，比如说限流要进行一个调整。另外，从SRE的角度来讲，整个压测的只能保证整个服务性能上它是没有什么问题的。到最后我们还需要引入一些**混沌工程(chaos enginnering)**、**故障注入**(AHAS)这些东西去进行一个演练,保证服务能够做到及时的故障响应。

![What is chaos engineering? | Dynatrace news](https://marvel-b1-cdn.bc0a.com/f00000000236551/dt-cdn.net/wp-content/uploads/2022/05/chaos-engineering.png)

![图1：AHAS服务体系.png](https://ucc.alicdn.com/pic/developer-ecology/8b4858ed58444ec7ae96a3568e525293.png)

**结果存档，有迹可寻**

完成了整个压测后，需要把整个压测的过程和结果记录进行存档，可以以列表的方式进行记录。因为，后面有一些大的变更之后，后面的大促还要继续去做压测，后面的压测也要依据每一次的压测结果进行参考。

**紧急预案，有的放矢**

接下来需要给团队一些紧急的预案，去做一些兜底的方案，这样就形成了一个**《大促作战手册》**，可能在一些紧急情况下并不一定能够按照这个作战手册来进行实施。但是，在梳理的过程当中其实能够发现很多风险点，发现之后可以及时抛出来去准备一些后门工具(脚本)在紧急情况下去做一个数据订正，然后再去完善整个监控系统，做好数据监控和埋点，然后有一个统一的监控大盘。真正在大促的时候只需要集体关注这个大盘就可以了。

最后，整个压测其实最难的是对于场景和需求的一个预期，因为这一步预估的这个流量一旦跟真实的情况差太多，那后面很多基于这个数据去进行压测都是有问题的，所以，前期在做压测的时候一定要对业务场景有比较深刻的理解和认识，多做一些兜底的方案，防患于未然总是没有问题的。

